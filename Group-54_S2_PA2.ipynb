{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# **PA2: Classification**\n",
    "\n",
    "### **Total : 1000 marks**\n",
    "\n",
    "</center>\n",
    "\n",
    "**Group Number:**\n",
    "\n",
    "Name:\n",
    "\n",
    "Roll Number:\n",
    "\n",
    "Name:\n",
    "\n",
    "Roll Number:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "- Follow along with the notebook, filling out the necessary code where instructed.\n",
    "\n",
    "- <span style=\"color: red;\">Make sure to run all cells for credit.</span>\n",
    "\n",
    "- <span style=\"color: red;\">Do not remove any pre-written code (unless explicitly mentioned in the cell that you are allowed to do so).</span>\n",
    "\n",
    "- <span style=\"color: red;\">You must attempt all parts.</span>\n",
    "\n",
    "## Submission Guidelines\n",
    "\n",
    "You are to submit a zip file containing the following files:\n",
    "\n",
    "1. This juptyer notebook. `Group<group_number>_S<section_number>_PA2.ipynb`\n",
    "2. Python file of the juptyer notebook. `Group<group_number>_S<section_number>_PA2.py`\n",
    "\n",
    "For example if I am from S1 and my group number is 25, my files would be named `Group25_S1_PA2.ipynb` and `Group25_S1_PA2.py`\n",
    "\n",
    "IMPORTANT: **Do not zip the files.** Submit both the files seperately, without zipping them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: k-NNs from Scratch (400 marks)\n",
    "\n",
    "You are <span style=\"color: red;\">not allowed</span> to use scikit-learn or any other machine learning toolkit for this part. You have to implement your own k-NN classifier from scratch.\n",
    "\n",
    "### Importing Libraries\n",
    "\n",
    "All of the necessary libraries for this part have been imported for you below. You may not use any other library apart from standard Python librares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idx2numpy in c:\\users\\feefu\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\feefu\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from idx2numpy) (2.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\feefu\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from idx2numpy) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "!pip install idx2numpy\n",
    "import idx2numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Extracting the dataset\n",
    "\n",
    "The MNIST dataset consists of 70,000 labeled images of handwritten digits, each of size 28 pixels by 28 pixels, yielding a total of 784 pixels per picture.\n",
    "\n",
    "Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This value ranges from 0-255\n",
    "\n",
    "The dataset can be downloaded from [here](https://www.kaggle.com/datasets/hojjatk/mnist-dataset) and is also available to in your assignment directory. The four relevant files in the folder are:\n",
    "\n",
    "- train-images-idx3-ubyte: training set images\n",
    "- train-labels-idx1-ubyte: training set labels\n",
    "- t10k-images-idx3-ubyte: test set images\n",
    "- t10k-labels-idx1-ubyte: test set labels\n",
    "\n",
    "The dataset has been split with 60,000 images in the train set, and the remaning 10,000 images in the test set.\n",
    "\n",
    "Your very first task is to to convert this dataset into a pandas dataframe.\n",
    "\n",
    "Hint: _use the idx2numpy package to convert the dataset to a multidimensional numpy array. The documentation can be visited [here](https://pypi.org/project/idx2numpy/). The resulting array then has to be flattened._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: (60000, 28, 28)\n",
      "Train labels: (60000,)\n",
      "Test images: (10000, 28, 28)\n",
      "Test labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Input the file paths\n",
    "train_images_path = \"mnist/train-images.idx3-ubyte\"\n",
    "train_labels_path = \"mnist/train-labels.idx1-ubyte\"\n",
    "test_images_path = \"mnist/t10k-images.idx3-ubyte\"\n",
    "test_labels_path = \"mnist/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "# TODO: Convert the idx files to numpy\n",
    "train_images=idx2numpy.convert_from_file(train_images_path)\n",
    "train_labels=idx2numpy.convert_from_file(train_labels_path)\n",
    "test_images=idx2numpy.convert_from_file(test_images_path)\n",
    "test_labels=idx2numpy.convert_from_file(test_labels_path)\n",
    "\n",
    "# TODO: Print the shape of the multidimensional array for both train and test set\n",
    "print(\"Train images:\",train_images.shape)\n",
    "print(\"Train labels:\",train_labels.shape)\n",
    "print(\"Test images:\",test_images.shape)\n",
    "print(\"Test labels:\",test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: (60000, 785)\n",
      "Train labels: (60000, 1)\n",
      "Test images: (10000, 785)\n",
      "Test labels: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Flatten the array, append the labels and print the shape again\n",
    "# The shape of your train set should be (60000, 785), and test set should be (10000, 785)\n",
    "# IMPORTANT: If your shapes are not matching, dont attempt the rest of the assignment unless you get it right.\n",
    "train_images=train_images.reshape(train_images.shape[0],-1) ##make row wise\n",
    "train_labels=train_labels.reshape(train_labels.shape[0],-1)\n",
    "test_images=test_images.reshape(test_images.shape[0],-1)\n",
    "test_labels=test_labels.reshape(test_labels.shape[0],-1)\n",
    "train_images=np.append(train_images,train_labels,axis=1) ##put the labels in the end, new syntax caconates \n",
    "test_images=np.append(test_images,test_labels,axis=1)\n",
    "print(\"Train images:\",train_images.shape)\n",
    "print(\"Train labels:\",train_labels.shape)\n",
    "print(\"Test images:\",test_images.shape)\n",
    "print(\"Test labels:\",test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What does each row of the dataset represents?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. It represents a single image from the given dataset since first it was 28x28 and now we've moved it into a single array by flattening it into a single row of 784 pixels plus a label \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Visualizing and preprocessing the dataset\n",
    "\n",
    "Now that we have a dataset to work with, we need to preprocess it further, before we pass it through our classifier. In this step, we will be seperating out the labels from the inputs, and attempt to standardize or normalize our dataset.\n",
    "\n",
    "Note that the standardization of a variable $x$ refers to:\n",
    "\n",
    "$$\n",
    "x' = \\frac{x - μ}{σ}\n",
    "$$\n",
    "\n",
    "where $μ$ is the mean of the variable and $σ$ is the standard deviation.\n",
    "\n",
    "On the other hand, variable normalization usually involves scaling the data to a specific range.\n",
    "\n",
    "You can read more about this [here](https://www.simplilearn.com/normalization-vs-standardization-article).\n",
    "\n",
    "After you've loaded and split the dataset, let's display some images. You can reshape these 784 values for each image, into a `28x28` array, then use either `matplotlib` or `PIL` to display the image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Extract labels and features\n",
    "train_x =train_images[:,:-1]\n",
    "train_y =train_images[:,-1]\n",
    "test_x =test_images[:,:-1]\n",
    "test_y =test_images[:,-1]\n",
    "\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAACdJJREFUeJzt3U+IVWUDx/Hn6igKFogmhNQsDEqQsIWbEQUbk4goQ9voKoRwEdoqXAiuRBAX1koiaNVoGxcJKrSoVSm60AI3ShsNxJyN4Z9ipvOi5E/qfRf3Obxz7jj38wFhGO6PcxCdL+fO+NhrmqYpAFBKmTfoGwBg9hAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQYWt9//33p9Xr/89e5c+cGfXswECODuSzMHnv27Cnr1q37x+deeumlgd0PDJIoMPQ2bNhQtm/fPujbgFnB20dQSvn999/L1NTUoG8DBk4UGHoffPBBefbZZ8uiRYvKpk2bysWLFwd9SzAw3j5iaC1cuLBs27atvPXWW2X58uXlypUr5ciRI4/eTvrhhx/Ka6+9NuhbhM71/Cc78MS1a9fKq6++WjZu3FjOnj076NuBznn7CP71U0fvvvtu+e6778r09PSgbwc6JwrwLy+88EL5888/y927dwd9K9A5UYB/+eWXXx5903nJkiWDvhXonCgwtH777bf/+tzly5fLN998U7Zs2VLmzfPXg+HjG80Mrddff70sXry4jI2NlRUrVjz66aPPP/+8LFiwoPz4449l9erVg75F6JwoMLQ+++yz8tVXXz36iaM7d+6U5557royPj5cDBw445oKhJQoAhDdNAQhRACBEAYAQBQBCFAAIUQCg/ujsh/9vLQBPr37+BYInBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYuTJhzDcli1b1snmlVdeKW1s3ry5evP8889Xb7Zt21a60Ov1Wu1+/vnn6s2mTZuqN5OTk2UYeVIAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACAfiMeutXbu2erN3797qzfr166s3q1atKnNN0zSdXOfChQutdjdv3qzePPPMM9WbSQfiATDsRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIB+LRmTYHzj10+vTp6s2SJUvKbDU1NdVqd+/everNF198Ub35+uuvqze3b9+u3ty6dat09ftA/zwpABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABBOSaUz77zzTqtdVyee/vXXX9WbTz/9tHpz+PDh0kbbU0WhhicFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgOg1TdOUPvR6vX5expBYuXJl9eby5cutrrV06dLShf3791dvDh06NCP3AjOhny/3nhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAYuTJh9C/3bt3z9qD7R46efJk9ebw4cMzci/wNPGkAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAOxKOVsbGxzq5179696s3BgwerN9PT09UbmGs8KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQTkll1rt161b15tdff52Re4G5zpMCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQPSapmlKH3q9Xj8vY0i8//771Zvjx4+3ulabP3sTExPVmw8//LB6c//+/eoNDEo/X+49KQAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEA/HozKFDh1rtPvnkk9KF8+fPV2/GxsZm5F5gJjgQD4AqogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEA/HozPz581vtTp48Wb15++23qzfT09PVmzNnzlRvdu3aVdq4fft2qx085kA8AKqIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABAOxGPWGx0d7eSgupdffrl04csvv2y1++ijj6o3Dx48aHUt5iYH4gFQRRQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAwimpzEkvvvhi9ebs2bOz9mTVh3bu3Fm9OXHixIzcC08np6QCUEUUAAhRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgHAgHvxtdHS0evPtt99Wb1atWlXaaHO4XZtD9Ji7HIgHQBVRACBEAYAQBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAGLkyYcw3K5fv169+emnnzo7EG98fLzVDmp4UgAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIB+LB37Zs2VK9ee+990pXTp8+3dm1GF6eFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQCi1zRNU/rQ6/X6eRn83y1fvrx6s2/fvurNnj17qjfz58+v3ly9erW0sXnz5urNjRs3Wl2LuamfL/eeFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBEAYAQBQBCFAAIp6TOMR9//HH15s0336zenDp1qnqzYsWK0sbWrVurN2vWrCld+OOPP6o3Y2Njra516dKlVjt4zCmpAFQRBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAEIUAAhRACBGnnzIXNDm4MI33nijk81sd+XKlerN7t27qzcOtmM286QAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEL2maZoyQwet0b1ly5ZVb44ePVq92bFjR/VmamqqtHHs2LHqzbVr16o3ExMT1ZvJycnqDQxKP1/uPSkAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhAPxAIZE40A8AGqIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgCEKAAQogBAiAIAIQoAhCgAEKIAQIgCACEKAIQoABCiAECIAgAhCgDESOlT0zT9vhSAp5QnBQBCFAAIUQAgRAGAEAUAQhQACFEAIEQBgBAFAMpj/wEvT25X4WY+9gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Implement a function to display image. The label should be used as a title\n",
    "#      Randomly pick 5 rows from the training dataset and display their images\n",
    "def display_image(features, label):\n",
    "    '''\n",
    "      Takes a 1D numpy array, reshapes to a 28x28 array and displays the image\n",
    "    '''\n",
    "    image =features.reshape(28,28)##reshaped to 28x28\n",
    "    plt.imshow(image, cmap='gray')##already displaying\n",
    "    plt.title(f\"{label}\") \n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "random_index =np.random.choice(train_x.shape[0],5)\n",
    "\n",
    "for i in random_index:\n",
    "    display_image(train_x[i],train_y[i])\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# TODO: normalize the data so that it falls in the [0, 1] range.\n",
    "#      Hint: the max value of each pixel is 255\n",
    "\n",
    "def normalize(data):\n",
    "    '''\n",
    "      scales the data to the range [0, 1]\n",
    "    '''\n",
    "    data=data/255\n",
    "    return data\n",
    "\n",
    "\n",
    "train_x =normalize(train_x)\n",
    "test_x =normalize(test_x)\n",
    "\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)\n",
    "\n",
    "\n",
    "# print(\"train_x min & max:\",np.min(train_x),np.max(train_x))  \n",
    "# print(\"test_x min & max:\",np.min(test_x),np.max(test_x))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. With the variable standardization formula shown above in the description, is this technique feasible in this dataset? Explain in detail.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. standardization is mostly used on data that has a varying range (for instance with salary, height, etc), whereas MNIST deals with pixel values that range from 0-255. Standardization also centers the data around zero which can cause negative pixel values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.3: Implementing k-NN Classifier\n",
    "\n",
    "Now you can create your own k-NN classifier. You can use the following steps as a guide:\n",
    "\n",
    "1. For a test data point, find its distance from all training instances.\n",
    "\n",
    "2. Sort the calculated distances in ascending order based on distance values.\n",
    "\n",
    "3. Choose k training samples with minimum distances from the test data point.\n",
    "\n",
    "4. Return the _most frequent_ class of these samples.\n",
    "\n",
    "For values of `k` where a tie occurs, you need to break the tie by backing off to the `k-1` value. In case there is still a tie, you will continue decreasing `k` until there is a clear winner.\n",
    "\n",
    "#### Important\n",
    "\n",
    "**Note:** Your function should work with _Euclidean_ distance as well as _Manhattan_ distance. Pass the distance metric as a parameter in the k-NN classifier function. Your function should also let one specify the value of `k`.\n",
    "\n",
    "**Note:** Your approach should be vectorized. Failure to implement a vectorization-based method to calculate distances will result in significant loss of marks. You can read up vectorization [here](https://towardsdatascience.com/vectorization-implementation-in-machine-learning-ca652920c55d)\n",
    "\n",
    "#### Distance functions\n",
    "\n",
    "Implement separate functions for the Euclidean and Manhattan distances. Formulas for both are given below.\n",
    "\n",
    "$$\n",
    "d_{\\text{Euclidean}}(\\vec{p},\\vec{q}) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + (p_3 - q_3)^2 + ... + (p_n - q_n)^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d_{\\text{Manhattan}}(\\vec{p},\\vec{q}) = |(p_1 - q_1)| + |(p_2 - q_2)| + |(p_3 - q_3)| + ... + |(p_n - q_n)|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Complete the following method functions:\n",
    "\n",
    "- `euclidean_distance`\n",
    "- `manhattan_distance`\n",
    "- `fit`\n",
    "- `get_neighbors`\n",
    "- `predict`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the class below\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, k):\n",
    "        # DO NOT EDIT !#\n",
    "        '''\n",
    "          Initializes the class\n",
    "        '''\n",
    "\n",
    "        self.k = k\n",
    "        self.train_x = None\n",
    "        self.train_y = None\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        # USE NUMPY METHODS\n",
    "        '''\n",
    "          Takes two numpy arrays and calculates the euclidean distance between them\n",
    "        '''\n",
    "        distance=np.sqrt(np.sum((x1-x2)**2))\n",
    "        return distance\n",
    "\n",
    "    def manhattan_distance(self, x1, x2):\n",
    "        # USE NUMPY METHODS\n",
    "        '''\n",
    "          Takes two numpy arrays and calculates the manhattan distance between them\n",
    "        '''\n",
    "        distance=np.sum(np.abs(x1-x2))\n",
    "        return distance\n",
    "\n",
    "    def fit(self, train_x, train_y):\n",
    "        '''\n",
    "          Stores the training dataset\n",
    "        '''\n",
    "        self.train_x=train_x\n",
    "        self.train_y=train_y\n",
    "        return self\n",
    "\n",
    "    def get_neighbors(self, new_point, distancefunc):\n",
    "        '''\n",
    "          Takes a new point and returns the k nearest neighbors\n",
    "          Hint: Sort the distances by their index to get the labels easily\n",
    "        '''\n",
    "\n",
    "        if distancefunc == 'euclidean':\n",
    "            distances = np.array([self.euclidean_distance(new_point, x) for x in self.train_x])\n",
    "        else:\n",
    "            distances = np.array([self.manhattan_distance(new_point, x) for x in self.train_x])\n",
    "\n",
    "        distance=np.argsort(distances)\n",
    "        # print(f\"Distances: {distances}\")  \n",
    "        # print(f\"Neighbors: {distance}\")\n",
    "        return distance[:self.k]\n",
    "\n",
    "    def predict(self, test_x, distancefunc):\n",
    "        '''\n",
    "          Takes a test set and returns the predicted labels\n",
    "        '''\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(test_x.shape[0]):\n",
    "            neighbors=self.get_neighbors(test_x[i],distancefunc)\n",
    "            labels=[self.train_y[j] for j in neighbors]\n",
    "            # print(f\"Labels of neighbors:{labels}\")\n",
    "            freq={} ##want to do a hashing array type thing\n",
    "            for label in labels:\n",
    "                freq[label]=freq.get(label,0)+1\n",
    "            mostfreq=max(freq,key=freq.get)\n",
    "            predictions.append(mostfreq)\n",
    "            \n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.4: Evaluation\n",
    "\n",
    "Now that you've created a model and \"trained\" it, you can move on to the Evaluation phase.\n",
    "\n",
    "- We will be implementing an `evaluate` function that computes the Confusion Matrix, Accuracy, and Macro-Average F1 score of your classifier. You can use multiple helper functions to calculate the individual metrics.\n",
    "\n",
    "- The function should take as input the predicted labels and the true labels. This will be built in steps: its easier to create a Confusion Matrix, then calculate things like the Precision, Recall and F1 from it.\n",
    "\n",
    "- We will also implement a function that displays our confusion matrix as a heatmap annotated with the data values.\n",
    "- The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n",
    "- You can have a look at some examples of heatmaps [here](https://seaborn.pydata.org/generated/seaborn.heatmap.html). (You don't have to use the seaborn libray, but it has some pretty colour palettes to choose from.)\n",
    "\n",
    "We recommend that you do not use hard coding in this function.\n",
    "\n",
    "---\n",
    "\n",
    "Complete the following functions:\n",
    "\n",
    "- `accuracy`\n",
    "- `make_confusion_matrix`\n",
    "- `make_heat_map`\n",
    "- `precision`\n",
    "- `recall`\n",
    "- `f1_score`\n",
    "- `macro_average_f1`\n",
    "- `evaluate`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, true_labels):\n",
    "    '''\n",
    "      Takes the predicted labels and the true labels and returns the accuracy\n",
    "    '''\n",
    "\n",
    "    correct =np.sum(predicted_labels==true_labels)\n",
    "    total =len(true_labels)\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "def make_confusion_matrix(predicted_labels, true_labels):\n",
    "    '''\n",
    "      Takes the predicted labels and the true labels and returns the confusion matrix\n",
    "      Hint: You can create a helper function which calculates each row of the confusion matrix\n",
    "    '''\n",
    "    confusionmatrix=np.zeros((2,2),dtype=int)\n",
    "    for i in range(len(true_labels)):\n",
    "        if true_labels[i]==0 and predicted_labels[i]==0:\n",
    "            confusionmatrix[0,0]+=1\n",
    "        elif true_labels[i]==0 and predicted_labels[i]==1:\n",
    "            confusionmatrix[0,1]+=1\n",
    "        elif true_labels[i]==1 and predicted_labels[i]==0:\n",
    "            confusionmatrix[1,0]+=1\n",
    "        elif true_labels[i]==1 and predicted_labels[i]==1:\n",
    "            confusionmatrix[1,1]+=1\n",
    "\n",
    "    return confusionmatrix\n",
    "\n",
    "\n",
    "def make_heat_map(confusion_matrix, title):\n",
    "    '''\n",
    "      Takes the confusion matrix and plots it as a heatmap\n",
    "    '''\n",
    "    # This function has already been implemented\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()\n",
    "    return None\n",
    "\n",
    "\n",
    "def precision(confusion_matrix, class_label):\n",
    "    '''\n",
    "    Takes the confusion matrix and a label and returns the precision for that class label\n",
    "    '''\n",
    "    if class_label==1:\n",
    "        true_positives=confusion_matrix[1, 1]  \n",
    "        false_positives=confusion_matrix[0, 1]  \n",
    "    else:\n",
    "        true_positives=confusion_matrix[0, 0]  \n",
    "        false_positives=confusion_matrix[1, 0]  \n",
    "\n",
    "    return (true_positives/(true_positives+false_positives) if (true_positives+false_positives)!=0 else 0)\n",
    "\n",
    "\n",
    "\n",
    "def recall(confusion_matrix, class_label):\n",
    "    '''\n",
    "    Takes the confusion matrix and a label and returns the recall for that class label\n",
    "    '''\n",
    "    if class_label==1:\n",
    "        true_positives=confusion_matrix[1, 1]  \n",
    "        false_negatives = confusion_matrix[1, 0]  \n",
    "    else:\n",
    "        true_positives=confusion_matrix[0, 0]  \n",
    "        false_negatives=confusion_matrix[0, 1] \n",
    "\n",
    "    return (true_positives/(true_positives+false_negatives) if (true_positives+false_negatives)!=0 else 0)\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    '''\n",
    "      Takes the precision and recall and returns the f1 score\n",
    "    '''\n",
    "    # This function has already been implemented\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "\n",
    "def macro_average_f1(confusion_matrix):\n",
    "    '''\n",
    "    Calculates the macro-average F1 score from a provided confusion matrix, over all classes\n",
    "    '''\n",
    "    # This function has already been implemented\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    f1_scores = []\n",
    "    for class_label in range(num_classes):\n",
    "        prec = precision(confusion_matrix, class_label)\n",
    "        rec = recall(confusion_matrix, class_label)\n",
    "        f1 = f1_score(prec, rec)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "\n",
    "def evaluate(predicted_labels, true_labels):\n",
    "    '''\n",
    "    Displays and returns a nicely formatted report with accuracy, macro-average f1 score, and confusion matrix\n",
    "    '''\n",
    "    accuracy_value=accuracy(predicted_labels,true_labels)\n",
    "    confusion_matrix=make_confusion_matrix(predicted_labels,true_labels)\n",
    "    macro_f1_value = macro_average_f1(confusion_matrix)\n",
    "    # print(f\"Accuracy:{accuracy_value*100:.2f}%\")\n",
    "    # print(f\"Macro-average F1 Score:{macro_f1_value:.2f}\")\n",
    "    # print(\"Confusion Matrix:\")\n",
    "    # print(confusion_matrix)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_value,\n",
    "        \"macro_f1\": macro_f1_value,\n",
    "        \"confusion_matrix\": confusion_matrix\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.5: k-fold Cross Validation\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://global.discourse-cdn.com/dlai/original/3X/a/3/a3ed2de61c2b4fa00f1b7e939753e1a7e181afb0.png\">\n",
    "</center>\n",
    "\n",
    "Now with the basics done, you can move on to the next step: k-fold Cross Validation. This is a more robust way of evaluating your model since it uses all the data for training and testing (effectively giving you `k` chances to verify the generalizability of your model).\n",
    "\n",
    "Now, implement a function that performs `k`-fold cross-validation on the training data for a specified value of `k`.\n",
    "\n",
    "In Cross Validation, you divide the dataset into `k` parts. `k-1` parts will be used for training and `1` part will be used for validation. You will repeat this process `k` times, each time using a different part for validation. You will then average the results of each fold to get the final result. Take a look at the image above for a better understanding.\n",
    "\n",
    "The function should return **predictions** for the **entire training data** (size of list/array should be equal to the size of the dataset). This is the result of appending the predicted labels for each validation-train split into a single list/array. Make sure the order of the predicted labels matches the order of the training dataset, so that they may directly be passed to your `evaluate` function together with the actual labels.\n",
    "\n",
    "---\n",
    "\n",
    "Complete the following functions:\n",
    "\n",
    "- `k_fold_split`\n",
    "- `k_fold_cross_validation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_split(num_folds, cv_no, train_x, train_y):\n",
    "    '''\n",
    "    Creates the train and test splits based off the value of k\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mum_folds : int\n",
    "        Number of folds\n",
    "    cv_no : int\n",
    "        The current fold number\n",
    "    train_x : nparray\n",
    "        The features\n",
    "    train_y : nparray\n",
    "        The labels\n",
    "    '''\n",
    "\n",
    "    num_samples =len(train_x)\n",
    "    fold_size =num_samples//num_folds\n",
    "    val_start =(cv_no-1)*fold_size\n",
    "    train_x_split=np.concatenate((train_x[:val_start],train_x[val_start+fold_size:]),axis=0)\n",
    "    train_y_split=np.concatenate((train_y[:val_start],train_y[val_start+fold_size:]),axis=0)\n",
    "    test_x_split=train_x[val_start:val_start+fold_size]\n",
    "    test_y_split=train_y[val_start:val_start+fold_size]\n",
    "\n",
    "    return train_x_split,train_y_split,test_x_split,test_y_split\n",
    "\n",
    "\n",
    "def k_fold_cross_validation(num_folds, k, train_x, train_y, distanceFunction):\n",
    "    \"\"\"\n",
    "    Returns the predictions for all the data points in the dataset using k-fold cross validation\n",
    "\n",
    "    num_folds: int\n",
    "      Number of folds\n",
    "    k: int\n",
    "      Number of neighbours to consider (hyperparameter)\n",
    "    train_x : nparray\n",
    "        The features\n",
    "    train_y : nparray\n",
    "        The labels\n",
    "    distanceFunction : str\n",
    "        Distance metric specified (manhattan / euclidean)\n",
    "    \"\"\"\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    for cv_no in range(1, num_folds + 1):\n",
    "        train_x_split,train_y_split,test_x_split,test_y_split=k_fold_split(num_folds,cv_no,train_x,train_y)\n",
    "        knn=KNN(k)\n",
    "        knn.fit(train_x_split,train_y_split)\n",
    "        predictions=knn.predict(test_x_split,distanceFunction)\n",
    "        all_predictions=np.append(all_predictions,predictions)\n",
    "\n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run your cross-validation function on the training data using `5-fold cross validation` for the values of `k = [1, 2, 3, 4, 5]`.\n",
    "\n",
    "Do this for both the Euclidean distance and the Manhattan distance for each value of `k`.\n",
    "\n",
    "Also run your evaluation function for each value of `k` (for both distance metrics) and print out the classification accuracy and F1 score.\n",
    "\n",
    "**Note: Save your evaluation stats for plotting later**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[6. 9. 7. ... 6. 3. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Save scores here for plotting later\n",
    "accuracy_list_euclidean = []\n",
    "f1_list_euclidean = []\n",
    "accuracy_list_manhattan = []\n",
    "f1_list_manhattan = []\n",
    "\n",
    "# For K fold cross validation you can randomly sample a subset of training dataset to conduct your cross validation on.\n",
    "# The size of this dataset should be 14000.\n",
    "# This should significantly cut down the run time.\n",
    "# This has already been implemented for you in this cell\n",
    "# Now use sampled_train_x and sampled_train_y in the next cell\n",
    "\n",
    "data = np.hstack((train_x, train_y.reshape(-1, 1)))\n",
    "sampled_data = data[np.random.choice(data.shape[0], 14000, replace=False)]\n",
    "sampled_train_x = sampled_data[:, :-1]\n",
    "sampled_train_y = sampled_data[:, -1]\n",
    "\n",
    "print(sampled_train_x)\n",
    "print(sampled_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1\n",
      "formula:euclidean\n"
     ]
    }
   ],
   "source": [
    "# TODO: Perform cross-validation for both distances and run your evaluation function for each K, printing the accuracy and macro-average F1 score.\n",
    "num_folds = 5\n",
    "k_values = [1, 2, 3, 4, 5]\n",
    "accuracyE=[]\n",
    "accuracyM=[]\n",
    "f1E=[]\n",
    "f1M=[]\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"K={k}\")\n",
    "    for distance in ['euclidean','manhattan']:\n",
    "        print(f\"formula:{distance}\")\n",
    "        predictions=k_fold_cross_validation(num_folds,k,sampled_train_x,sampled_train_y,distance)\n",
    "        results=evaluate(predictions,sampled_train_y)\n",
    "        print(f\"Accuracy:{results['accuracy']*100:.2f}%\")\n",
    "        print(f\"Macro-average F1 Score:{results['macro_f1']:.2f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(results['confusion_matrix'])\n",
    "        if distance=='euclidean':\n",
    "            accuracyE.append(results['accuracy'])\n",
    "            f1E.append(results['macro_f1'])\n",
    "        else:\n",
    "            accuracyM.append(results['accuracy'])\n",
    "            f1M.append(results['macro_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, present the results as a graph with `k` values on the x-axis and classification accuracy on the y-axis. Use a single plot to compare the two versions of the classifier (one using Euclidean and the other using Manhattan distance metric).\n",
    "\n",
    "Make another graph but with the F1-score on the y-axis this time. The graphs should be properly labeled on axes, with a title, and a legend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a graph with k values on the x-axis and classification accuracy on the y-axis (both distances on one plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot a graph with k values on the x-axis and F1-score on the y-axis (both distances on one plot)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.6: Prediction\n",
    "\n",
    "Finally, use the best value of `k` for both distance metrics and run it on the test dataset.\n",
    "\n",
    "Find the confusion matrix, classification accuracy and F1 score and print them.\n",
    "\n",
    "The confusion matrix must be displayed as a heatmap annotated with the data values. The axes should be properly labelled and the colormap used needs to be shown next to the heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test with the best K for euclidean distance\n",
    "best_k = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test with the best K for Manhattan distance\n",
    "best_k = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Decision Tree (350 Marks)\n",
    "\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a test on a feature. Each leaf node represents a class label. The paths from the root to leaf represent classification rules.\n",
    "\n",
    "Use entropy as the measure of impurity and calculate the information gain to split the nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries for Part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing The Dataset for Part 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Loading the dataset\n",
    "\n",
    "Read the dataset provided in the file `data.csv` using the pandas library.\n",
    "\n",
    "1. First, print the number of rows and columns (shape) in the dataset.\n",
    "2. Print the first 5 rows of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the dataset from the csv file using pandas. The file is called 'data.csv'\n",
    "\n",
    "# TODO: Print the shape of the data\n",
    "\n",
    "# TODO: Print the first 5 rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Visualizing the dataset\n",
    "\n",
    "Plot a scatter plot of the data. Be sure to label the axes, give the plot a title, and include a legend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Visualize the data using a scatter plot. Be sure to include a legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Splitting the dataset into train and test sets\n",
    "\n",
    "Split the dataset into a training set and a testing set. You should have 80% of the data in the training set and 20% of the data in the test set. You are **not** allowed to use any library to do this.\n",
    "\n",
    "After splitting, print the number of rows in the training set and the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split the dataset into train and test sets without using sklearn\n",
    "# use a 80/20 split\n",
    "\n",
    "# TODO: print the number of rows in the train and test sets\n",
    "\n",
    "# Calculate the number of rows for training and testing sets\n",
    "total_rows = ...\n",
    "train_size = ...\n",
    "test_size = ...\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_set = ...\n",
    "test_set = ...\n",
    "\n",
    "# Print the number of rows in the training and test sets\n",
    "print(\"Number of rows in the training set:\", len(train_set))\n",
    "print(\"Number of rows in the test set:\", len(test_set))\n",
    "\n",
    "print(train_set)\n",
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the features and the target variable in the training and testing set into `X_train`, `y_train`, `X_test`, and `y_test`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Separate into X and Y\n",
    "X_train = ...\n",
    "y_train = ...\n",
    "X_test = ...\n",
    "y_test = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following `plot_decision_boundary` function for plotting decision boundaries in Part 2 of the assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not modify the code below\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    '''\n",
    "    Function to plot the decision boundary of a classification model.\n",
    "    Parameters:\n",
    "        model: the classification model\n",
    "        X: the input features\n",
    "        y: the target labels\n",
    "    '''\n",
    "\n",
    "    # Set min and max values and give it some padding\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = 0.05\n",
    "\n",
    "    # Generate a grid of points with distance h between them\n",
    "    x1, x2 = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
    "                         np.arange(x2_min, x2_max, h))\n",
    "\n",
    "    # Predict the function value for the whole grid\n",
    "    # Convert list to NumPy array\n",
    "    Z = np.array(model.predict(np.c_[x1.ravel(), x2.ravel()]))\n",
    "    Z = Z.reshape(x1.shape)\n",
    "\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(x1, x2, Z, cmap=plt.cm.Set1, alpha=0.4)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='.',\n",
    "                s=20, linewidth=1, alpha=0.2, cmap=plt.cm.Set1)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.title(\"Dataset\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4: Decision Tree Classifier\n",
    "\n",
    "This class will serve as the decision tree classifier. It should have the following methods:\n",
    "\n",
    "- **init**: This is the constructor of the class. It initializes the class with the maximum depth of the tree. The default value of the maximum depth should be 5, but the user should be able to change it.\n",
    "- **fit**: This method takes the training data and the training labels as arguments. It should build the decision tree using the training data and labels. You are to use the algorithm discussed in the class to build the decision tree.\n",
    "- **predict**: This method takes the test data and returns the predicted labels for the test data.\n",
    "\n",
    "Feel free to add any other methods that you think might be useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build a decision tree classifier from scratch\n",
    "# NOTE: You are allowed to alter method signatures and create as many helper functions as needed\n",
    "# NOTE: You can also use a completely different implementation than what is stated below\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, left=None, right=None, feature=None, threshold=None, classifier=None):\n",
    "        self.left = ...\n",
    "        self.right = ...\n",
    "        self.classifier = ...\n",
    "        self.feature = ...\n",
    "        self.threshold = ...\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5):\n",
    "        self.max_depth = ...\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, X_y_train, depth=0):\n",
    "        self.root = self.make_tree(X_y_train, depth)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = np.array([])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def traverse_tree(self, data_point, node):\n",
    "        pass\n",
    "\n",
    "    def make_tree(self, X_y_train, depth):\n",
    "        pass\n",
    "\n",
    "    def make_leaf_node(self, X_y_train):\n",
    "\n",
    "        return None\n",
    "\n",
    "    def best_split(self, X_y_train):\n",
    "        best_split_info = {\n",
    "            'feature': None,\n",
    "            'threshold': None,\n",
    "            'info_gain': -np.inf,\n",
    "            'data_left': None,\n",
    "            'data_right': None\n",
    "        }\n",
    "\n",
    "        return None\n",
    "\n",
    "    def split(self, X_y_train, feature_index, threshold):\n",
    "\n",
    "        return None\n",
    "\n",
    "    def calculate_entropy(self, node):\n",
    "\n",
    "        return None\n",
    "\n",
    "    def calculate_information_gain(self, parent, left_child, right_child):\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5: Evaluation\n",
    "\n",
    "Evaluate the Decision Tree classifier using the testing set. Print the accuracy, precision, recall, and F1 score of the classifier just like you did for the KNN classifier. Also, plot the confusion matrix of the classifier, as a heatmap.\n",
    "\n",
    "You can also import and use pprint to print and visualize the decision tree. Although it is not mandatory, it can be useful to understand the decision tree better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n",
      "Precision: 0.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a DecisionTree object and train it using the training data\n",
    "dt = ...\n",
    "\n",
    "# TODO: Make predictions on the test data\n",
    "dt_y_pred = ...\n",
    "\n",
    "# TODO: Evaluate the model using accuracy, precision, recall, and f1 score\n",
    "accuracy = ...\n",
    "precision = ...\n",
    "recall = ...\n",
    "f1 = ...\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a confusion matrix and plot it using a heatmap.\n",
    "# You can use sklearn's confusion_matrix function to create the matrix\n",
    "# and seaborn's heatmap function to plot it. The libraries has been imported for you\n",
    "\n",
    "conf_matrix = ...\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6: Plot Decision Boundary\n",
    "\n",
    "Similar to the KNN classifier, plot the decision boundary for the decision tree classifier. Also, plot the decision boundary along with a scatter plot of the **Entire Data**. You are supposed to use the `plot_decision_boundary` function given to you at the start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the decision boundary on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.7: Different Max depth values\n",
    "\n",
    "After seeing how our initial model performed with a default value of Max depth , we will now explore different Max depth values. This step will help us find the best Max depth setting to improve our model's ability to predict accurately\n",
    "\n",
    "- Use the `plot_decision_boundary` function given above to plot the decision boundaries and scatter plots for different values of `max_depth` from 1 to 25.\n",
    "- Gather the accuracy for each `max_depth` value on the test set and compare them. Show these comparisons in a graph to easily see which `max_depth` value leads to the best predictions. This will help us choose the best k value for our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the decision boundary on the test set for different values of max_depth from 1 to 25\n",
    "\n",
    "model_accuracies = []\n",
    "\n",
    "for max_depth in range(1, 25):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the accuracies of the model for different values of max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. What can you conclude from you results? Explain your observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Naive Bayes (250 Marks)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this part, you will be implementing Naive Bayes model based on the dataset features and task requirements.\n",
    "\n",
    "For reference and additional details, please go through [Chapter 4](https://web.stanford.edu/~jurafsky/slp3/) of the SLP3 book.\n",
    "\n",
    "In this assignment, you are provided with `golf_data.csv`. Your task is to:\n",
    "\n",
    "1. Analyze the datasets and the dataset’s characteristics.\n",
    "2. Implement both **Naive Bayes** from scratch, adhering to the guidelines below regarding allowed libraries.\n",
    "3. Finally, apply the corresponding models using the `sklearn` library and compare the results with your own implementation.\n",
    "\n",
    "### Guidelines:\n",
    "\n",
    "- Use only **numpy** and **pandas** for the manual implementation of Naive Bayes classifier. No other libraries should be used for this part.\n",
    "- For the final part of the assignment, you will use **sklearn** to compare your implementation results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All necessary libraries for this assignment have already been added. You are not allowed to add any additional imports.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "\n",
    "# Third-party library imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1: Loading the Datasets\n",
    "\n",
    "In this assignment, you are provided with the dataset:\n",
    "\n",
    "- Golf Dataset (available in CSV format in the given folder)\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "**Golf Dataset**: You can find the CSV file of the Golf Dataset in the resources provided with this assignment. This dataset aims to explore factors that influence the decision to play golf, which could be valuable for predictive modeling tasks. ​​\n",
    "\n",
    "You can read more about Bernoulli Naive Bayes [here](https://medium.com/@gridflowai/part-2-dive-into-bernoulli-naive-bayes-d0cbcbabb775).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "golf_data = pd.read_csv('file path')  # Replace with correct path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2: Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Golf Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will apply one-hot encoding to the categorical columns of the Golf dataset (you can use `pd's` `get_dummies`) and split the data into training and test sets. You can use `sklearn's` `train_test_split` which has been imported for you above. Ensure that the `test_size` parameter is set to 0.3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3: Implementing Naive Bayes from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli Naive Bayes\n",
    "\n",
    "### From Scratch\n",
    "\n",
    "Recall that the Bernoulli Naive Bayes model is based on **Bayes' Theorem**:\n",
    "\n",
    "$$\n",
    "P(y \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)}\n",
    "$$\n",
    "\n",
    "What we really want is to find the class \\(c\\) that maximizes \\(P(c \\mid x)\\), so we can use the following equation:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c \\mid x) = \\underset{c}{\\text{argmax}} \\ P(x \\mid c)P(c)\n",
    "$$\n",
    "\n",
    "In the case of **Bernoulli Naive Bayes**, we assume that each word \\(x_i\\) in a sentence follows a **Bernoulli distribution**, meaning that the word either appears (1) or does not appear (0) in the document. We can simplify the formula using this assumption:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ P(c) \\prod_{i=1}^{n} P(x_i = 1 \\mid c)^{x_i} P(x_i = 0 \\mid c)^{1 - x_i}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $x_i = 1$ if the $i^{\\text{th}}$ word is present in the document.\n",
    "- $x_i = 0$ if the $i^{\\text{th}}$ word is not present in the document.\n",
    "\n",
    "We can estimate $P(c)$ by counting the number of times each class appears in our training data, and dividing by the total number of training examples. We can estimate $P(x_i = 1 \\mid c)$ by counting the number of documents in class $c$ that contain the word $x_i$, and dividing by the total number of documents in class $c$.\n",
    "\n",
    "### **Important: Laplace Smoothing**\n",
    "\n",
    "When calculating $P(x_i = 1 \\mid c)$ and $P(x_i = 0 \\mid c)$, we apply **Laplace smoothing** to avoid zero probabilities. This is essential because, without it, any word that has not appeared in a document of class $c$ will have a probability of zero, which would make the overall product zero, leading to incorrect classification.\n",
    "\n",
    "**Reason**: Laplace smoothing ensures that we don't encounter zero probabilities by adding a small constant (typically 1) to both the numerator and the denominator. This is particularly useful when a word has never appeared in the training data for a specific class.\n",
    "\n",
    "The smoothed probability formula is:\n",
    "\n",
    "$$\n",
    "P(x_i = 1 \\mid c) = \\frac{\\text{count of documents in class } c \\text{ where } x_i = 1 + 1}{\\text{total documents in class } c + 2}\n",
    "$$\n",
    "\n",
    "This ensures no word has a zero probability, even if it was unseen in the training data.\n",
    "\n",
    "### Avoiding Underflow with Logarithms:\n",
    "\n",
    "To avoid underflow errors due to multiplying small probabilities, we apply logarithms, which convert the product into a sum:\n",
    "\n",
    "$$\n",
    "\\hat{c} = \\underset{c}{\\text{argmax}} \\ \\log P(c) + \\sum_{i=1}^{n} \\left[ x_i \\log P(x_i = 1 \\mid c) + (1 - x_i) \\log P(x_i = 0 \\mid c) \\right]\n",
    "$$\n",
    "\n",
    "You will now implement this algorithm.\n",
    "\n",
    "<span style=\"color: red;\"> For this part, the only external library you will need is `numpy`. You are not allowed to use anything else.</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "class BNB:\n",
    "    def __init__(self, X, y):\n",
    "        self.cond_probs = {}\n",
    "        self.classes = ...\n",
    "        self.class_counts = ...\n",
    "        self.priors = {}\n",
    "        self.samples, _ = X.shape\n",
    "        self.priors = ...\n",
    "\n",
    "    def model_fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def model_predict(self, X):\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use your implementation to train a Naive Bayes model on the training data, and generate predictions for the Validation Set.\n",
    "\n",
    "Report the Accuracy, Precision, Recall, and F1 score of your model on the validation data. Also display the Confusion Matrix. You are allowed to use `sklearn.metrics` for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WELL DONE! You have successfully completed the Assignment! 🎉💯\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
